{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ctaylor\\Anaconda3\\envs\\spacy_v2\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import joblib\n",
    "import re\n",
    "import functools as ft\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "import gensim\n",
    "from tqdm import *\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer, LabelEncoder, PolynomialFeatures\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.word2vec.KeyedVectors.load('vectors/gutenberg_en_300.vec')\n",
    "\n",
    "logistic_models = joblib.load('logistic_models_for ensembling.pkl.gz')\n",
    "\n",
    "# `logistic_models` is a list of (estimator, test score) tuples\n",
    "clf_list = [x[0] for x in logistic_models]\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(['animate', 'inanimate'])\n",
    "\n",
    "preprocessor = make_pipeline(\n",
    "    Normalizer(norm='max'),\n",
    "    PolynomialFeatures(include_bias=False)\n",
    ")\n",
    "\n",
    "nouns_with_vectors = {w for w in model.wv.vocab if w.endswith('|NOUN')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_vec(word):\n",
    "    return get_vecs(list(word))\n",
    "\n",
    "\n",
    "def get_vecs(words):\n",
    "    assert isinstance(words, list)\n",
    "    _vecs = []\n",
    "    \n",
    "    for w in words:\n",
    "        _vecs.append(model.wv.word_vec(w))\n",
    "        \n",
    "    return np.array(_vecs)\n",
    "\n",
    "\n",
    "def avg_predict_proba(vec, estimators):\n",
    "    \"\"\"Adapted from https://stackoverflow.com/a/42925296.\n",
    "    \"\"\"\n",
    "    _predictions = []\n",
    "    \n",
    "    for _clf in estimators:\n",
    "        # `predict_proba` below is expecting a 2D array, but `vec` is only 1D.\n",
    "        _expanded = np.expand_dims(vec, axis=0)\n",
    "        _prediction = _clf.predict_proba(_expanded)\n",
    "        _predictions.append(_prediction)\n",
    "    \n",
    "    _pred_array = np.asarray(_predictions)\n",
    "    \n",
    "    _avg_prediction = np.average(_pred_array, axis=0)\n",
    "    \n",
    "    return _avg_prediction[0]\n",
    "\n",
    "\n",
    "def pipeline(vec, preprocessor, estimators, label_encoder):\n",
    "    _preprocess_x = preprocessor.fit_transform(vec)\n",
    "    \n",
    "    _pred_proba = np.apply_along_axis(\n",
    "        avg_predict_proba,\n",
    "        axis=1,\n",
    "        arr=_preprocess_x,\n",
    "        **dict(estimators=estimators)\n",
    "    )\n",
    "    \n",
    "    _pred_class = np.argmax(_pred_proba, axis=1)\n",
    "    _pred_label = label_encoder.inverse_transform(_pred_class)\n",
    "    \n",
    "    return _pred_label\n",
    "\n",
    "\n",
    "predict_animacy = ft.partial(\n",
    "    pipeline,\n",
    "    **dict(preprocessor=preprocessor,\n",
    "           estimators=clf_list,\n",
    "           label_encoder=le)\n",
    ")\n",
    "\n",
    "\n",
    "def make_xml_tag(span, parsed_doc, tag_lookup, ent_label):\n",
    "    _xml_id = '#' + span.text_with_ws.strip().lower().replace(' ', '_')\n",
    "\n",
    "    span.merge()\n",
    "    _idx = span.start\n",
    "    _token = parsed_doc[_idx]\n",
    "\n",
    "    _token._.set('xml_tag', tag_lookup[label])\n",
    "    _token._.set('xml_attrs', {'id': _xml_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('noun_clusters_to_keep.txt') as fo:\n",
    "    denoised_nouns = {utils.clean_word(w.strip()) for w in fo.read().split()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ent_lookup = {\n",
    "    'PERSON': 'persName', \n",
    "    'NORP': 'personGrp', \n",
    "    'GPE': 'personGrp', \n",
    "    'LOC': 'placeName'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spacy.tokens.Token.set_extension('xml_tag')\n",
    "spacy.tokens.Token.set_extension('xml_attrs')\n",
    "spacy.tokens.Token.set_extension('is_person', default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(r'data/texts/cato_minor.txt') as fo:\n",
    "    text = fo.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for _chunk in doc.noun_chunks:\n",
    "    is_person = False\n",
    "    \n",
    "    for _word in _chunk:\n",
    "        _norm_word = utils.clean_word(_word.text + '|' + _word.pos_)\n",
    "        \n",
    "        if (_word.pos_ == 'NOUN'\n",
    "                and _norm_word in denoised_nouns\n",
    "                and _norm_word in model.wv.vocab):\n",
    "            _word_vec = np.expand_dims(model.wv.word_vec(_norm_word), axis=0)\n",
    "            _prediction = predict_animacy(_word_vec)\n",
    "\n",
    "            if _prediction == 'animate':\n",
    "                is_person = True\n",
    "                break\n",
    "    \n",
    "    if is_person:\n",
    "        xml_id = '#' + _chunk.text_with_ws.strip().lower().replace(' ', '_')\n",
    "        \n",
    "        _chunk.merge()\n",
    "        idx = _chunk.start\n",
    "        token = doc[idx]\n",
    "        \n",
    "        token._.set('xml_tag', ent_lookup['PERSON'])\n",
    "        token._.set('xml_attrs', {'id': xml_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ents = (ent for ent in doc.ents if ent.label_ in ent_lookup)\n",
    "\n",
    "for e in ents:\n",
    "    xml_id = '#' + e.text_with_ws.strip().lower().replace(' ', '_')\n",
    "    \n",
    "    e.merge()\n",
    "    \n",
    "    idx = e.start\n",
    "    token = doc[idx]\n",
    "    \n",
    "    token._.set('xml_tag', ent_lookup[e.label_])\n",
    "    token._.set('xml_attrs', {'id': xml_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_with_markup = []\n",
    "\n",
    "for i, sent in enumerate(doc.sents):\n",
    "    for token in sent:\n",
    "        display = ''\n",
    "        attr_repr = ''\n",
    "\n",
    "        tag = token._.get('xml_tag')\n",
    "        attrs = token._.get('xml_attrs')\n",
    "\n",
    "        if attrs:\n",
    "            attr_repr = ''.join([' ' + a + '=\"' + attrs[a] + '\"' for a in attrs])\n",
    "\n",
    "        if tag and token.text.strip():\n",
    "            markup = '<' + tag + attr_repr + '>' + token.text_with_ws + '</' + tag + '>'\n",
    "        else:\n",
    "            markup = token.text_with_ws\n",
    "\n",
    "        text_with_markup.append(markup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "space_tag_re = re.compile(r' (</[A-Za-z]+>)')\n",
    "space_re = re.compile(r' +')\n",
    "\n",
    "clean_xml = space_tag_re.sub(r'\\1 ', ''.join(text_with_markup))\n",
    "clean_xml = space_re.sub(r' ', clean_xml)\n",
    "\n",
    "with open('data/texts/cato_minor.xml', 'w') as xml_out:\n",
    "    xml_out.write(clean_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_spacy_v2)",
   "language": "python",
   "name": "conda_spacy_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
